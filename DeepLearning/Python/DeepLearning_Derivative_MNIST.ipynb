{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invisible-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_data = np.loadtxt('mnist_train.csv', delimiter = \",\", dtype = np.float32)\n",
    "test_data = np.loadtxt('mnist_test.csv', delimiter = \",\", dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "talented-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "# 수치미분 함수\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.000\n",
    "    grad = np.zeros_like(x) # x 크기의 어레이 선언 및 초기화\n",
    "    \n",
    "    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx] # x 바뀌기 때문에 저장해둠\n",
    "        x[idx]= float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respected-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    # 생성자\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes):\n",
    "        \n",
    "        self.input_nodes = input_nodes     # input_nodes = 784\n",
    "        self.hidden_nodes = hidden_nodes     # hidden_nodes = 100\n",
    "        self.output_nodes = output_nodes     # output _nodes = 10\n",
    "        \n",
    "        # 2층 hidden layer unit\n",
    "        # 가중치 W, 바이어스 b 초기화\n",
    "        self.W2 = np.random.rand(self.input_nodes, self.hidden_nodes)      # W2 = (784 x 100)\n",
    "        self.b2 = np.random.rand(self.hidden_nodes)                        # b2 = (100,)\n",
    "        \n",
    "        # 3층 output layer unit\n",
    "        self.W3 = np.random.rand(self.hidden_nodes, self.output_nodes)     # W3 = (100 x 10)\n",
    "        self.b3 = np.random.rand(self.output_nodes)                        # b3 = (10,)\n",
    "        \n",
    "        # Learning Rate 초기화\n",
    "        self.learning_rate = 1e-4\n",
    "        \n",
    "    \n",
    "    # feed forward 를 통하여 입력층부터 출력층까지 데이터 전달, 손실함수(cross-entropy) 값 계산\n",
    "    def feed_forward(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z1 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        y1 = sigmoid(z1)\n",
    "        \n",
    "        z2 = np.dot(y1, self.W3) + self.b3\n",
    "        y = sigmoid(z2)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )    \n",
    "    \n",
    "    def loss_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z1 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        y1 = sigmoid(z1)\n",
    "        \n",
    "        z2 = np.dot(y1, self.W3) + self.b3\n",
    "        y = sigmoid(z2)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )    \n",
    "    \n",
    "    # input_data : 784개, target_data : 10개\n",
    "    def train(self, training_data):\n",
    "        \n",
    "        # 0 ~ 1 사이 값으로 정규화(Nomalization)\n",
    "        # 만약 정답이 5라면 index 5번째는 0.99, 나머지는 0.01로 정규화\n",
    "        self.target_data = np.zeros(output_nodes) + 0.01\n",
    "        self.target_data[int(training_data[0])] = 0.99\n",
    "        \n",
    "        # 입력 데이터가 0 ~ 255여서 overflow 발생 가능 -> 모든 입력 0 ~ 1로 정규화\n",
    "        self.input_data = (training_data[1:] / 255.0 * 0.99) + 0.01\n",
    "        \n",
    "        f = lambda x : self.feed_forward()\n",
    "        \n",
    "        self.W2 -= self.learning_rate * numerical_derivative(f, self.W2)\n",
    "    \n",
    "        self.b2 -= self.learning_rate * numerical_derivative(f, self.b2)\n",
    "        \n",
    "        self.W3 -= self.learning_rate * numerical_derivative(f, self.W3)\n",
    "    \n",
    "        self.b3 -= self.learning_rate * numerical_derivative(f, self.b3)\n",
    "   \n",
    "    \n",
    "    # query, 즉 미래 값 예측 함수\n",
    "    def predict(self, xdata):\n",
    "        \n",
    "        z1 = np.dot(input_data, self.W2) + self.b2       \n",
    "        y1 = sigmoid(z1)                               \n",
    "        \n",
    "        z2 = np.dot(y1, self.W3) + self.b3         \n",
    "        y2 = sigmoid(z2)                      \n",
    "    \n",
    "        predicted_num = np.argmax(y)     # 가장 큰 값을 가지는 인덱스를 정답으로 인식\n",
    "    \n",
    "        return y, result\n",
    "    \n",
    "    # 정확도 측정함수\n",
    "    def accuracy(self, test_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        not_matched_list = []\n",
    "        \n",
    "        for index in range(len(test_data)):\n",
    "            \n",
    "            label = int(test_data[index, 0])\n",
    "            \n",
    "            # Nomalization\n",
    "            data = (test_data[index, 1:] / 255.0 * 0.99) + 0.01\n",
    "            \n",
    "            predicted_num = self.predict(data)\n",
    "            \n",
    "            if label == predicted_num:\n",
    "                matched_list.append(index)\n",
    "            else :\n",
    "                not_matched_list.append(index)\n",
    "                \n",
    "        print(\"Current Acurracy =\", 100*(len(matched_list)/len(test_data), \" %\"))\n",
    "            \n",
    "        return matched_list, not_matched_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 loss_val = 143.77341309854825\n",
      "step = 1 loss_val = 143.77341309854825\n",
      "step = 2 loss_val = 143.77341309854825\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 784\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10\n",
    "\n",
    "nn = NeuralNetwork(input_nodes, hidden_nodes, output_nodes)\n",
    "\n",
    "for step in range(30001):     # 전체 training data의 50%\n",
    "    \n",
    "    index = np.random.randint(0, len(training_data)-1)\n",
    "    \n",
    "    nn.train(training_data[index])\n",
    "    \n",
    "    print(\"step =\", step, \"loss_val =\", nn.loss_val())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-excess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-reynolds",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
